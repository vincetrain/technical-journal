[Install Elasticsearch with a Debian package](https://www.elastic.co/docs/deploy-manage/deploy/self-managed/install-elasticsearch-with-debian-package)

[Create or update roles (Elasticsearch)](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-put-role)

[Secure communication with an on-premise Elasticsearch cluster](https://www.elastic.co/docs/reference/logstash/secure-connection#es-security-onprem)  

[Secure communication with Logstash (Filebeat)](https://www.elastic.co/docs/reference/beats/filebeat/configuring-ssl-logstash)

[Logstash grok pattern for apache error log](https://discuss.elastic.co/t/logstash-grok-pattern-for-apache-error-log/337676)

[Grok Filter Configuration Options (Logstash)](https://www.elastic.co/docs/reference/logstash/plugins/plugins-filters-grok#plugins-filters-grok-options)

[Apache module (Filebeat)](https://www.elastic.co/docs/reference/beats/filebeat/filebeat-module-apache)

[logstash-patterns-core](https://github.com/logstash-plugins/logstash-patterns-core/blob/main/patterns/ecs-v1/httpd)


**NOTE: this follows the installation of the ELK stack using apt repositories. It may be more worth installing each component of the ELK stack using the official archives instead as it avoids any potential issues with system permissions.**
## Preparing apt Repository
Install signing key
```sh
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
```

Install apt repository
```sh
sudo apt-get install apt-transport-https
echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/9.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-9.x.list
sudo apt-get update
```

If installing an Elastic app with `apt`, binaries will typically be inside `/usr/share/[app]/bin` (i.e `/usr/share/elasticsearch/bin`) and configuration files will be inside of `/etc/[app]` (i.e `/etc/elasticsearch/`).
## Installing & Configuring Elasticsearch
Before installing Elasticsearch, **ensure the machine's hostname is the hostname that is desired**! Elasticsearch autogenerates certificates for HTTPS usage when installed. **Mismatching hostnames from hostname changes after installation will mess this up!**

We can now install the `elasticsearch` package with apt.

**Successful installation will show the `elastic` user's in shell. Consider noting it down or adding it to an environment variable**

Check Elasticsearch health
```sh
curl --cacert /path/to/http_ca.crt -u [username] https://localhost:9200/_cluster/health
```
> `http_ca.crt` is usually auto-generated by Elasticsearch. This will be in the `/config/certs` or `/etc/elasticsearch/certs`.

For Elasticsearch to work, ensure at least 1 correct initial master node is set inside `elasticsearch.yml`.
```yml
node.name: example-node-1
...
cluster.initial_master_nodes: ["example-node-1"]
```
>This solves error status 503, `"master_not_discovered_exception"`
## Installing & Configuring Kibana
We can install the `kibana` package with apt or via the latest archive on the Elastic website.

The Kibana webserver will be running at [localhost:5601](localhost:5601)

Create a user. This is the user we will use with Kibana. You will be prompted to enter a password for the user after running this command.
```sh
elasticsearch/bin/elasticsearch-users useradd [user]
```

Add required user roles for unlimited Kibana and Elasticsearch usage
```sh
elasticsearch/bin/elasticsearch-users roles [user] -a kibana_admin,superuser
```
>The `kibana_admin` role allows unlimited to usage of Kibana, including administrative use. 
>The `superuser` role allows unlimited read and write access (and usage) of Elasticsearch and its indices.

Create enrollment token for usage with Kibana. This will be pasted inside of the Kibana web page when prompted after a fresh installation.
```shell
elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana --url https://[url-of-elasticsearch]:9200
```
>`-s` indicates the scope, and can either be `kibana` or `node`
>`--url` indicates the URL to use for connection to Elasticsearch.

Gets verification code for Kibana enrollment.
```sh
bin/kibana-verification-code
```
### Setup SSL with Kibana
Generate local CA using elasticsearch-certutil
```sh
elasticsearch/bin/elasticsearch-certutil 
```
### Stack Monitoring
To setup stack monitoring run the `bin/kibana-encryption-keys` command, and add the generated keys to `/config/kibana.yml`
```sh
bin/kibana-encryption-keys generate
```

After this is done, stack monitoring can be configured easily in Kibana's "Management>Stack Monitoring" section.
## Installing & Configuring Logstash

Logstash should be installed on its own node, or at least separate from the application being logged it can be hardware intensive due to its complexity.

System logs are stored at `/var/log/`. To allow Logstash to access these logs, give the `logstash` user the `adm` role.
```sh
usermod -aG adm logstash
```

For Logstash, we add an Elasticsearch role with required permissions.
```HTTP
POST /security/role/logstash_user
{
	"description": "Grants necessary privileges for logstash functionality",
	"cluster": ["all"],
	"indices": [
		{
			"names": ["your-index"],
			"privileges": ["all"]
		}
	]
}
```
Refer to [[#Installing & Configuring Kibana]] to setup a user for Logstash

Because Elasticsearch was autoconfigured with SSL, we need to copy Elasticsearch's SSL certificate to our Logstash instance.
```sh
mkdir /path/to/logstash/certs/
cp /path/to/http_ca.crt /path/to/logstash/certs/
## you may also want to chmod or chown the cert depending on how you're using logstash
```
> `http_ca.crt` will be the same certificate file that was autogenerated with Elasticsearch.
> `/path/to/logstash/certs/` refers to the directory where you want to store your Logstash. I typically keep them in `config/certs/`.

### Stack Monitoring
To monitor Logstash, we can use Metricbeat.

By default, Logstash exposes its HTTP API in localhost and is not secured by SSL. Because of this, Metricbeat should be installed on the same node as Logstash.

To further secure our Logstash API, we can define some basic authentication credentials in `logstash.yml`
```yaml
api.auth.type: basic
api.auth.basic.username: "username"
api.auth.basic.password: "supersecurepassword"
```

By default, Metricbeat modules can be accessed in the `/modules.d` directory, or `/etc/metricbeat/modules.d` 

`metricbeat.yml` Can be found in the install directory of Metricbeat, or `/etc/metricbeat`

After installing Metricbeat, we can use the built in Logstash module to monitor Logstash.
```bash
metricbeat modules list
metricbeat module enable logstash-xpack
```
> Consider disabling system metrics `metricbeat modules disable system`

Modify the logstash-xpack.

### Logstash Pipelines
Logstash pipelines are, by default, stored in `/etc/logstash/conf.d`, but it is possible to store pipelines elsewhere.

To run pipelines at a specified directory, use the `-f` flag and point it towards the pipeline path.
> It may be worth adding `/usr/share/logstash/bin` to PATH for convenience.

If using Vim, ensure the following configurations are set for tabs to be recognized and usable in pipelines
```vimrc
set tabstop=4
set shiftwidth=4
set expandtab
```

Empty Logstash configuration template
```conf
input {

}

filter {

}

output {

}

```

Make Logstash read from a file
```conf
input {
	file {
		path => "/path/to/logs"
		# Optional, define sincedb_path as null to re-read ALL logs every run
		start_position => "beginning"
		sincedb_path => "/dev/null"
	}
}
```
>sincedb_path should be "NUL" on Windows

Make Logstash output to Elasticsearch with SSL using previous certs
```conf
output {
	elasticsearch {
		hosts => ["https://elasticsearch-url:9200"]
		ssl_enabled => true
		ssl_certificate_authorities => ["/etc/logstash/certs/http_ca.crt"]
		user => "logstash_user"
		password => "your-password-here"
		index => "example-index"
	}
}
```
> Also consider adding the `logstash` group to the user who will be running Logstash if running via its binary (requires re-login/reboot to work).
## Filebeat Configuration

For Filebeat to work over SSL, use `elasticsearch/bin/elasticsearch-certutil` to create a certificate.

Configure `filebeat.yml` to use these certificates and output to Logstash.
```yml
output.logstash:
	hosts: ["logstash.server:5044"]
	ssl.certificate_authorities: ["/path/to/ca.crt"]
	ssl.certificate: "/path/to/filebeat.crt"
	ssl.key: "/path/to/filebeat.key"
```

We can enable, list, or disable any modules using `filebeat modules`.
### Logging Apache2 Events
Apache has two types of logs (access and error), so we will create a pipeline that logs both errors into their own respective index.

Filebeat comes with the a module called Apache. Configure and enable this beforehand.

Before configuring any pipelines, ensure Logstash has a user with sufficient roles to manage indices in Elasticsearch.

First lets configure our Logstash pipeline to input from Filebeat
```conf
input {
  beats {
    port => 5044
    ssl_enabled => true
    ssl_certificate_authorities => ["/path/to/ca.crt"]
    ssl_certificate => "/path/to/logstash.crt"
    ssl_key => "/path/to/logstash.key"
    ssl_client_authentication => "required"
  }
}
```
> This opens port 5044 for Logstash to listen on. SSL authentication is required, and must match the CA certificate we provided

We can now configure the filter for Logstash to parse the logs with
```conf
filter {
	if [event][dataset] == "apache.access" {
		grok {
			match => {"message" => "%{HTTPD_COMBINEDLOG}"}
		}
	} else {
		grok {
			match => {"message" => "%{HTTPD_ERRORLOG}"}
		}
	}
}
```
> Note the usage of conditionals to filter for log type

Using the concept of conditionals above, we can put access and error logs into their own indices
```conf
output {
    if [event][dataset] == "apache.access" {
        elasticsearch {
            hosts => ["https://[url-of-elasticsearch]:9200"]
            ssl_enabled => true
            ssl_certificate_authorities => ["/path/to/http_ca.crt"]
            user => "logstash"
            password => "supersecurepassword"
            index => "apache-10-access"
        }
    }
    if [event][dataset] == "apache.error" {
        elasticsearch {
            hosts => ["https://[url-of-elasticsearch]:9200"]
            ssl_enabled => true
            ssl_certificate_authorities => ["/path/to/http_ca.crt"]
            user => "logstash"
            password => "supersecurepassword"
            index => "apache-10-error"
        }
    }
}
```

We can now enable the Apache module inside of Filebeat with `filebeat modules enable apache`. Make sure the Apache module is configured to have both access and error logging enabled.

To test that this pipeline is working, we can `curl` the Apache server to test access logging, and run `printf "BADREQUEST / HTTP/1.1\r\nHost: localhost\r\n\r\n" | nc localhost 80`  to test error logging.